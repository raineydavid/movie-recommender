{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Processing Coursework\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In this short notebook, we will load and explore the movielens dataset. Specifically, this notebook covers:\n",
    "\n",
    "Loading data in memory\n",
    "Creating SQLContext\n",
    "Creating Spark DataFrame\n",
    "Group data by columns\n",
    "Operating on columns\n",
    "Running SQL Queries from a Spark DataFrame\n",
    "Loading in a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a recommendation system which uses transactional data linking a user and an item to get a list of items to recommend to the user. There are 2 approaches:\n",
    "## Collaborative Filtering\n",
    "In the MovieLens dataset, there are movies previously rated by a user, and the view is to attempt to identify if users who previously behaved similarly, ie liked/ disliked similar movies in the past, will have similar behaviors in the future. The input information is a user and the output is a list of items and their associated score. \n",
    "## Content based Filtering\n",
    "In the MovieLens dataset, there are further information contained about each individual item, ie the movie and there are additional information supplied such as tags by users, genre information which can be used to further compare similar items.  The input information would be a model and the output information a list of items and their associated score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our approach\n",
    "For this project, item based collaborative filtering has been selected. By comparing every pair of items (movies X, Y), try to find users who rated both items. Create a vector for each item and calculate the correlation between these vectors. When someone rates an item, we take this as our input and recommend other items (movies) most correlated with that item. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's get the data that we will working with in this notebook. We are using two files from the MovieLens dataset.  As part of this project, data was already set up, however the way to obtain the data has been added for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget --quiet http://www.grouplens.org/system/files/ml-100k.zip | unzip -q -o -d /data/movie-ratings/ | hadoop fs -put - /data/movie-ratings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\r\n",
      "drwxr-xr-x   - alvarogr ECS640U          0 2017-12-11 11:52 /data/movie-ratings/cv\r\n",
      "drwxr-xr-x   - hdfs     bigdata          0 2015-12-01 10:57 /data/movie-ratings/ml-10M100K\r\n",
      "-rw-r--r--   3 hdfs     bigdata     522197 2015-12-01 10:17 /data/movie-ratings/movies.dat\r\n",
      "-rw-r--r--   3 hdfs     bigdata  265105635 2015-12-01 10:17 /data/movie-ratings/ratings.dat\r\n",
      "-rw-r--r--   3 hdfs     bigdata    3584119 2015-12-01 10:17 /data/movie-ratings/tags.dat\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls  '/data/movie-ratings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 items\r\n",
      "-rw-r--r--   3 hdfs bigdata      11135 2015-12-01 10:57 /data/movie-ratings/ml-10M100K/README.html\r\n",
      "-rw-r--r--   3 hdfs bigdata        753 2015-12-01 10:57 /data/movie-ratings/ml-10M100K/allbut.pl\r\n",
      "-rw-r--r--   3 hdfs bigdata     522197 2015-12-01 10:57 /data/movie-ratings/ml-10M100K/movies.dat\r\n",
      "-rw-r--r--   3 hdfs bigdata  265105635 2015-12-01 10:57 /data/movie-ratings/ml-10M100K/ratings.dat\r\n",
      "-rw-r--r--   3 hdfs bigdata       1092 2015-12-01 10:57 /data/movie-ratings/ml-10M100K/split_ratings.sh\r\n",
      "-rw-r--r--   3 hdfs bigdata    3584119 2015-12-01 10:57 /data/movie-ratings/ml-10M100K/tags.dat\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls  '/data/movie-ratings/ml-10M100K'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxr-xr-x   - alvarogr ECS640U          0 2017-12-11 11:52 /data/movie-ratings/cv/10-item\r\n",
      "drwxr-xr-x   - alvarogr ECS640U          0 2017-12-11 11:53 /data/movie-ratings/cv/5-fold\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls  '/data/movie-ratings/cv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\"\r\n",
      "  \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\">\r\n",
      "<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\" xml:lang=\"en\">\r\n",
      "  <head>\r\n",
      "    <meta http-equiv=\"Content-Type\" content=\"text/html;charset=utf-8\" />\r\n",
      "    <style type=\"text/css\">\r\n",
      "      h1 {\r\n",
      "        color:#fc3;\r\n",
      "        font-family:\"Lucida Grande\",Verdana,sans-serif; \r\n",
      "        font-size: 150%; \r\n",
      "        font-weight: normal; \r\n",
      "        margin:34px 0 0;\r\n",
      "        background-color: #7A0019;\r\n",
      "      }\r\n",
      "      p {\r\n",
      "        margin-left: 20px;\r\n",
      "      }\r\n",
      "      p.file_line_structure {\r\n",
      "        margin-left: 40px;\r\n",
      "      }\r\n",
      "      table {\r\n",
      "        margin-left: 30px;\r\n",
      "      }\r\n",
      "      th {\r\n",
      "        text-align:left;\r\n",
      "      }\r\n",
      "    </style>\r\n",
      "\r\n",
      "    <title>MovieLens 10M/100k Data Set README</title>\r\n",
      "  </head>\r\n",
      "  <body>\r\n",
      "    <h1>\r\n",
      "        Summary\r\n",
      "    </h1>\r\n",
      "    <p>\r\n",
      "      This data set contains 10000054 ratings and 95580 tags \r\n",
      "      applied to 10681 movies by 71567 users of the \r\n",
      "      online movie recommender service <a href=\"http://www.movielens.org\">MovieLens</a>.\r\n",
      "    </p>\r\n",
      "    <p>\r\n",
      "      Users were selected at random for inclusion.  All users selected had rated \r\n",
      "      at least 20 movies.  Unlike previous MovieLens data sets, no demographic \r\n",
      "      information is included.  Each user is represented by an id, and no other \r\n",
      "      information is provided.\r\n",
      "    </p>\r\n",
      "\r\n",
      "    <p>\r\n",
      "      The data are contained in three files, <code>movies.dat</code>, \r\n",
      "      <code>ratings.dat</code> and <code>tags.dat</code>.\r\n",
      "      Also included are scripts for generating subsets of the data to support five-fold\r\n",
      "      cross-validation of rating predictions.  More details about the contents and use\r\n",
      "      of all these files <a href=\"#file_desc\">follows</a>.\r\n",
      "    </p>\r\n",
      "\r\n",
      "    <p>\r\n",
      "      This and other GroupLens data sets are publicly available for download at \r\n",
      "      <a href=\"http://www.grouplens.org/taxonomy/term/14\">GroupLens Data Sets</a>.\r\n",
      "    </p>\r\n",
      "    <h1>\r\n",
      "      Usage License\r\n",
      "    </h1>\r\n",
      "    <p>\r\n",
      "      Neither the University of Minnesota nor any of the researchers\r\n",
      "      involved can guarantee the correctness of the data, its suitability\r\n",
      "      for any particular purpose, or the validity of results based on the\r\n",
      "      use of the data set.  The data set may be used for any research\r\n",
      "      purposes under the following conditions:\r\n",
      "    </p>\r\n",
      "    <ul>\r\n",
      "        <li>The user may not state or imply any endorsement from the\r\n",
      "       University of Minnesota or the GroupLens Research Group.</li>\r\n",
      "\r\n",
      "        <li>The user must acknowledge the use of the data set in\r\n",
      "       publications resulting from the use of the data set, and must\r\n",
      "       send us an electronic or paper copy of those publications.</li>\r\n",
      "\r\n",
      "        <li>The user may not redistribute the data without separate\r\n",
      "       permission.</li>\r\n",
      "\r\n",
      "        <li>The user may not use this information for any commercial or\r\n",
      "       revenue-bearing purposes without first obtaining permission\r\n",
      "       from a faculty member of the GroupLens Research Project at the\r\n",
      "       University of Minnesota.</li>\r\n",
      "    </ul>\r\n",
      "    <p>\r\n",
      "      The executable software scripts are provided \"as is\" without warranty \r\n",
      "      of any kind, either expressed or implied, including, but not limited to, \r\n",
      "      the implied warranties of merchantability and fitness for a particular purpose. \r\n",
      "      The entire risk as to the quality and performance of them is with you. \r\n",
      "      Should the program prove defective, you assume the cost of all \r\n",
      "      necessary servicing, repair or correction.\r\n",
      "    </p>\r\n",
      "    <p>\r\n",
      "      In no event shall the University of Minnesota, its affiliates or employees \r\n",
      "      be liable to you for any damages arising out of the use or inability to use \r\n",
      "      these programs (including but not limited to loss of data or data being \r\n",
      "      rendered inaccurate).\r\n",
      "    </p>\r\n",
      "\r\n",
      "    <p>\r\n",
      "      If you have any further questions or comments, please email <a href='mailto:grouplens-info@cs.umn.edu'>grouplens-info</a>\r\n",
      "    </p>\r\n",
      "\r\n",
      "    <h1>\r\n",
      "        Acknowledgements\r\n",
      "    </h1>\r\n",
      "    <p>\r\n",
      "      Thanks to Rich Davies for generating the data set.\r\n",
      "    </p>\r\n",
      "\r\n",
      "    <h1>\r\n",
      "        Further Information About GroupLens\r\n",
      "    </h1>\r\n",
      "    <p>\r\n",
      "      <a href=\"http://www.grouplens.org/\">GroupLens</a> is a research group in the \r\n",
      "      <a href=\"http://www.cs.umn.edu/\">Department of Computer Science and Engineering</a>\r\n",
      "      at the <a href=\"http://www.umn.edu/\">University of Minnesota</a>.  Since its \r\n",
      "      inception in 1992, GroupLens' research projects have explored a variety of fields\r\n",
      "      including: \r\n",
      "    </p>\r\n",
      "    <ul>\r\n",
      "      <li>Information Filtering</li>\r\n",
      "      <li>Recommender Systems</li>\r\n",
      "      <li>Online Communities</li>\r\n",
      "      <li>Mobile and Ubiquitious Technologies</li>\r\n",
      "      <li>Digital Libraries</li>\r\n",
      "      <li>Local Geographic Information Systems.</li>\r\n",
      "    </ul>\r\n",
      "    <p>\r\n",
      "      GroupLens Research operates a movie recommender based on\r\n",
      "      collaborative filtering, <a href=\"http://www.movielens.org/\">MovieLens</a>,\r\n",
      "      which is the source of these data.\r\n",
      "    </p>\r\n",
      "\r\n",
      "    <h1 id=\"file_desc\">\r\n",
      "      Content and Use of Files\r\n",
      "    </h1>\r\n",
      "\r\n",
      "    <h2>\r\n",
      "      Character Encoding\r\n",
      "    </h2>\r\n",
      "    <p>\r\n",
      "      The three data files are encoded as \r\n",
      "      <a href=\"http://en.wikipedia.org/wiki/Utf-8\">UTF-8</a>.  This is a departure\r\n",
      "      from previous MovieLens data sets, which used different character encodings.\r\n",
      "      If accented characters in movie titles or tag values (e.g. MisÃ©rables, Les (1995))\r\n",
      "      display incorrectly, make sure that any program reading the data, such as a \r\n",
      "      text editor, terminal, or script, is configured for UTF-8.\r\n",
      "    </p>\r\n",
      "\r\n",
      "    <h2>\r\n",
      "      User Ids\r\n",
      "    </h2>\r\n",
      "    <p>\r\n",
      "      Movielens users were selected at random for inclusion.  Their ids have been \r\n",
      "      anonymized.\r\n",
      "    </p>\r\n",
      "    <p>\r\n",
      "      Users were selected separately for inclusion \r\n",
      "      in the ratings and tags data sets, which implies that user ids may appear in \r\n",
      "      one set but not the other.\r\n",
      "    </p>\r\n",
      "    <p>\r\n",
      "      The anonymized values are consistent between the ratings and tags data files.  \r\n",
      "      That is, user id <em>n</em>, if it appears in both files, refers to the same\r\n",
      "      real MovieLens user.\r\n",
      "    </p>\r\n",
      "\r\n",
      "    <h2>\r\n",
      "      Ratings Data File Structure\r\n",
      "    </h2>\r\n",
      "    <p>\r\n",
      "      All ratings are contained in the file <code>ratings.dat</code>.  Each line of this \r\n",
      "      file represents one rating of one movie by one user, and has the following format:\r\n",
      "    </p>\r\n",
      "    <p class=\"file_line_structure\">\r\n",
      "      <code>UserID::MovieID::Rating::Timestamp</code>\r\n",
      "    </p>\r\n",
      "    <p>\r\n",
      "      The lines within this file are ordered first by UserID, then, within user, \r\n",
      "      by MovieID.\r\n",
      "    </p>\r\n",
      "    <p>\r\n",
      "      Ratings are made on a 5-star scale, with half-star increments.\r\n",
      "    </p>\r\n",
      "    <p>\r\n",
      "      <a href=\"http://en.wikipedia.org/wiki/Unix_time\">Timestamps</a> represent \r\n",
      "      seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.\r\n",
      "    </p>\r\n",
      "\r\n",
      "    <h2>\r\n",
      "      Tags Data File Structure\r\n",
      "    </h2>\r\n",
      "    <p>\r\n",
      "      All tags are contained in the file <code>tags.dat</code>.  Each line of this \r\n",
      "      file represents one tag applied to one movie by one user, and has \r\n",
      "      the following format:\r\n",
      "    </p>\r\n",
      "    <p class=\"file_line_structure\">\r\n",
      "      <code>UserID::MovieID::Tag::Timestamp</code>\r\n",
      "    </p>\r\n",
      "    <p>\r\n",
      "      The lines within this file are ordered first by UserID, then, within user, \r\n",
      "      by MovieID.\r\n",
      "    </p>\r\n",
      "    <p>\r\n",
      "      <a href=\"http://en.wikipedia.org/wiki/Tag_(metadata)\">Tags</a> are user \r\n",
      "      generated metadata about movies.  Each tag is typically a single word, or\r\n",
      "      short phrase.  The meaning, value and purpose of a particular tag is \r\n",
      "      determined by each user.\r\n",
      "    </p>\r\n",
      "    <p>\r\n",
      "      <a href=\"http://en.wikipedia.org/wiki/Unix_time\">Timestamps</a> represent \r\n",
      "      seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.\r\n",
      "    </p>\r\n",
      "\r\n",
      "    <h2>\r\n",
      "      Movies Data File Structure\r\n",
      "    </h2>\r\n",
      "    <p>\r\n",
      "      Movie information is contained in the file <code>movies.dat</code>.  \r\n",
      "      Each line of this file represents one movie, and has the following format:\r\n",
      "    </p>\r\n",
      "    <p class=\"file_line_structure\">\r\n",
      "      <code>MovieID::Title::Genres</code>\r\n",
      "    </p>\r\n",
      "    <p>\r\n",
      "      MovieID is the real MovieLens id.\r\n",
      "    </p>\r\n",
      "    <p>\r\n",
      "      Movie titles, by policy, should be entered identically to those\r\n",
      "      found in <a href=\"http://www.imdb.com/\">IMDB</a>, including year of release.\r\n",
      "      However, they are entered manually, so errors and inconsistencies may exist.\r\n",
      "    </p>\r\n",
      "    <p>\r\n",
      "      Genres are a pipe-separated list, and are selected from the following:\r\n",
      "    </p>\r\n",
      "    <ul>\r\n",
      "\t  <li>Action</li>\r\n",
      "\t  <li>Adventure</li>\r\n",
      "\t  <li>Animation</li>\r\n",
      "\t  <li>Children's</li>\r\n",
      "\t  <li>Comedy</li>\r\n",
      "\t  <li>Crime</li>\r\n",
      "\t  <li>Documentary</li>\r\n",
      "\t  <li>Drama</li>\r\n",
      "\t  <li>Fantasy</li>\r\n",
      "\t  <li>Film-Noir</li>\r\n",
      "\t  <li>Horror</li>\r\n",
      "\t  <li>Musical</li>\r\n",
      "\t  <li>Mystery</li>\r\n",
      "\t  <li>Romance</li>\r\n",
      "\t  <li>Sci-Fi</li>\r\n",
      "\t  <li>Thriller</li>\r\n",
      "\t  <li>War</li>\r\n",
      "\t  <li>Western</li>\r\n",
      "    </ul>\r\n",
      "\r\n",
      "    <h2>\r\n",
      "      Cross-Validation Subset Generation Scripts\r\n",
      "    </h2>\r\n",
      "    <p>\r\n",
      "      A Unix shell script, <code>split_ratings.sh</code>, is provided that, if desired, \r\n",
      "      can be used to split the ratings data for five-fold cross-validation\r\n",
      "      of rating predictions.  It depends on a second script, allbut.pl, which \r\n",
      "      is also included and is written in Perl.  They should run without modification\r\n",
      "      under Linux, Mac OS X, Cygwin or other Unix like systems.\r\n",
      "    </p>\r\n",
      "    <p>\r\n",
      "      Running <code>split_ratings.sh</code> will use <code>ratings.dat</code>\r\n",
      "      as input, and produce the fourteen output files described below.  Multiple\r\n",
      "      runs of the script will produce identical results.\r\n",
      "    </p>\r\n",
      "    <table style=\"width:75%\" border=\"1\">\r\n",
      "      <tr>\r\n",
      "        <th style=\"width:25%\">File Names</th>\r\n",
      "        <th>Description</th>\r\n",
      "      </tr>\r\n",
      "      <tr>\r\n",
      "        <td>\r\n",
      "          r1.train, r2.train, r3.train, r4.train, r5.train<br/>\r\n",
      "          r1.test, r2.test, r3.test, r4.test, r5.test<br/>\r\n",
      "         </td>\r\n",
      "         <td>\r\n",
      "           The data sets r1.train and r1.test through r5.train and r5.test\r\n",
      "           are 80%/20% splits of the ratings data into training and test data.\r\n",
      "           Each of r1, ..., r5 have disjoint test sets; this if for\r\n",
      "           5 fold cross validation (where you repeat your experiment\r\n",
      "           with each training and test set and average the results).\r\n",
      "        </td>\r\n",
      "      </tr>\r\n",
      "      <tr>\r\n",
      "        <td>\r\n",
      "          ra.train, rb.train<br/>\r\n",
      "          ra.test, rb.test<br/>\r\n",
      "         </td>\r\n",
      "        <td>\r\n",
      "          The data sets ra.train, ra.test, rb.train, and rb.test\r\n",
      "          split the ratings data into a training set and a test set with\r\n",
      "          exactly 10 ratings per user in the test set.  The sets\r\n",
      "          ra.test and rb.test are disjoint.\r\n",
      "         </td>\r\n",
      "      </tr>\r\n",
      "    </table>\r\n",
      "    <p style=\"text-align:right\">\r\n",
      "      <a href=\"http://validator.w3.org/check?uri=referer\">\r\n",
      "        <img style=\"border:0;width:88px;height:31px\"\r\n",
      "          src=\"http://www.w3.org/Icons/valid-xhtml10\"\r\n",
      "          alt=\"Valid XHTML 1.0 Strict\" height=\"31\" width=\"88\" />\r\n",
      "      </a>\r\n",
      "\r\n",
      "      <a href=\"http://jigsaw.w3.org/css-validator/\">\r\n",
      "        <img style=\"border:0;width:88px;height:31px\"\r\n",
      "          src=\"http://jigsaw.w3.org/css-validator/images/vcss\"\r\n",
      "          alt=\"Valid CSS!\" />\r\n",
      "      </a>\r\n",
      "    </p>\r\n",
      "  </body>\r\n",
      "</html>\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -cat /data/movie-ratings/ml-10M100K/README.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing External files/LibrariesÂ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure the environment variables for loading sparkfrom jupyter notebook is setup correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import os\n",
    "os.environ['SPARK_HOME']='/usr/lib/spark'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use print function from python 3, use the from future command.  Ensure that jupyter notebook can find spark by using the findspark library, this references SPARK_HOME environment variable set up earlier.  Any external libraries imported need to be installed using pip install example, however if lacking admin permissions do pip install example --user  for example pip install sys --user.  Any error that has ImportError: No module named example means a pip install is required for a module named example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "from __future__ import print_function \n",
    "\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext,sql\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "import array\n",
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "import plotly\n",
    "from time import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(appName = \"MovieLens\").getOrCreate()\n",
    "sqlContext = sql.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for parsing movielens data and functions for comparing item similarity. Add the files to the spark context. Import the functions to be used here to use shorthand notation and not do filename.function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addPyFile('movielensfcn.py')\n",
    "sc.addPyFile('similarity.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from movielensfcn import parseMovies,parseRatings, removeDuplicates, itemItem\n",
    "from similarity import cosine_similarity, pearson_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Dataset (RDD) creation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Review the raw data files\n",
    "Inspect the files to see what we are dealing with. Review the contents of the movies.dat and ratings.dat files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_file = \"/data/movie-ratings/ratings.dat\"\n",
    "movies_file = \"/data/movie-ratings/movies.dat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the text file from Hadoop File System (HDFS) and return it as an RDD of strings. An RDD is a distibuted collection which is automatically distributed by spark and you can set how many partitions it is distributed over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_raw = sc.textFile(ratings_file)\n",
    "movies_raw = sc.textFile(movies_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the read me there are 10681 movies and 10000054 ratings.  Double check the data corresponds to this. and check timings for larger file set to see if optimization of partitions is required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 185.06 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1 loop, best of 3: 87.4 ms per loop\n",
      "There are 10681 rows in the /data/movie-ratings/movies.dat file\n"
     ]
    }
   ],
   "source": [
    "t0 = time\n",
    "numMovies = movies_raw.count()\n",
    "print('There are {0} rows in the {1} file'.format(numMovies,movies_file))\n",
    "tt = time() - t0\n",
    "print('Count completed in {0} seconds'.format(tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 20.1 s per loop\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'numRatings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-8073eee5207b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'timeit numRatings = ratings_raw.count()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'There are {0} rows in the {1} file'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumRatings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratings_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'numRatings' is not defined"
     ]
    }
   ],
   "source": [
    "t0 = time\n",
    "numRatings = ratings_raw.count()\n",
    "print('There are {0} rows in the {1} file'.format(numRatings, ratings_file))\n",
    "tt = time() - t0\n",
    "print('Count completed in {0} seconds'.format(tt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since results as expected.  Proceed with notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are approximately 1M records, it may be faster to set the number of partitions on spark.  Since the movie file is relatively small with approximately 10K records we can hold in memory using collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "numPartitions =1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using take(n) on the RDD to return an array of n elements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'1::122::5::838985046',\n",
       " u'1::185::5::838983525',\n",
       " u'1::231::5::838983392',\n",
       " u'1::292::5::838983421',\n",
       " u'1::316::5::838983392']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_raw.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'1::Toy Story (1995)::Adventure|Animation|Children|Comedy|Fantasy',\n",
       " u'2::Jumanji (1995)::Adventure|Children|Fantasy',\n",
       " u'3::Grumpier Old Men (1995)::Comedy|Romance',\n",
       " u'4::Waiting to Exhale (1995)::Comedy|Drama|Romance',\n",
       " u'5::Father of the Bride Part II (1995)::Comedy']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_raw.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li>There is no header file.</li>\n",
    "    <li>Notice that the columns are separated by :: </li>\n",
    "    <li>Ratings File Format: user::movie::rating::timestamp</li>\n",
    "    <li>Movies File format: movie::titleandyear::genre.  The genre fields are separated by |</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary investigations has shown that file ending with .dat are separated by ::, so split the contents to get the corresponding fields for user, movie, rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data will be split into 1000 partitions\n",
      "Processing the following files: \n",
      "Ratings:/data/movie-ratings/ratings.dat\n",
      "Movies:/data/movie-ratings/movies.dat\n"
     ]
    }
   ],
   "source": [
    "print('The data will be split into {0} partitions'.format(numPartitions))\n",
    "print('Processing the following files: \\nRatings:{0}\\nMovies:{1}'.format(ratings_file, movies_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (ratings_file.find('.dat') != -1):\n",
    "    movies_tmp= movies_raw.map(lambda line: re.split(r'::'))\n",
    "    movies= movies_tmp.map(lambda line: (int(line[0]),(line[1],line[2])))\n",
    "    ratings_tmp = ratings_raw.map(lambda line: re.split(r'::'))\\\n",
    "        .partitionBy(numPartitions)\n",
    "    ratings = ratings_tmp.map(lambda line: (int(line[0]),(int(line[1]),float(line[2]))))\\\n",
    "                         .partitionBy(numPartitions)\n",
    "else:\n",
    "    ratings_header = ratings_raw.take(1)[0]\n",
    "    movies_header = movies_raw.take(1)[0]\n",
    "    movies= movies_raw.filter(lambda line: line!=movies_header)\\\n",
    "                    .map(lambda line: re.split(r',',line)).map(lambda line: (int(line[1]),(line[0],line[2])))\n",
    "    ratings = ratings_raw.filter(lambda line: line!=ratings_header)\\\n",
    "                    .map(lambda line: re.split(r',',line))\\\n",
    "                    .map(lambda x: (int(line[1]),(int(line[0]),float(line[2]))))\\\n",
    "                    .partitionBy(numPartitions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 4 times, most recent failure: Lost task 0.3 in stage 12.0 (TID 40, 138.37.32.71, executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/serializers.py\", line 133, in dump_stream\n    for obj in iterator:\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1702, in add_shuffle_key\n  File \"<ipython-input-26-b4bea9263dba>\", line 4, in <lambda>\nTypeError: split() takes at least 2 arguments (1 given)\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:342)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:242)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1457)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1445)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1444)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1444)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1668)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1627)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1616)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1862)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1875)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1888)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:393)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/serializers.py\", line 133, in dump_stream\n    for obj in iterator:\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1702, in add_shuffle_key\n  File \"<ipython-input-26-b4bea9263dba>\", line 4, in <lambda>\nTypeError: split() takes at least 2 arguments (1 given)\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:342)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:242)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-ba696cf446a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mratings_tmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    937\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    940\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 813\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    307\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 4 times, most recent failure: Lost task 0.3 in stage 12.0 (TID 40, 138.37.32.71, executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/serializers.py\", line 133, in dump_stream\n    for obj in iterator:\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1702, in add_shuffle_key\n  File \"<ipython-input-26-b4bea9263dba>\", line 4, in <lambda>\nTypeError: split() takes at least 2 arguments (1 given)\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:342)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:242)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1457)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1445)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1444)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1444)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1668)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1627)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1616)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1862)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1875)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1888)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:393)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/serializers.py\", line 133, in dump_stream\n    for obj in iterator:\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1702, in add_shuffle_key\n  File \"<ipython-input-26-b4bea9263dba>\", line 4, in <lambda>\nTypeError: split() takes at least 2 arguments (1 given)\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:342)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:242)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "ratings_tmp.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the file has a header, strip it out by filtering the header contents.  Make the user the key for ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.toDF(['item_id', \"title\", \"genre\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratingsDF = ratings.toDF(['user_id','item','rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RatingsDF = ratings.toDF(['user_id','itemid_rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the type of RatingsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(RatingsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The printSchema() method gives more details about the DataFrameâ€™s schema and structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RatingsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RatingsDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many movies do we have in the movies file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numMovies = movies_tmp.values().map(lambda line: line[1]).distinct().count()\n",
    "print(\"number of movies: {0}\".format(numMovies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many users have rated our movies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numUsers = ratings_tmp.values().map(lambda line: line[0]).distinct().count()\n",
    "print(\"number of users: {0}\".format(numUsers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numRatings = ratings_tmp.count()\n",
    "print(\"total number of ratings: {0}\".format(numRatings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-rated Items\n",
    "Firstly join the ratings against itself to get all pairs of co-rated items (movies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings_data = ratings.join(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join two dataframes and get only one 'item_id' column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings_data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratingsDF2 = sqlContext.createDataFrame(user_ratings_data,['user_id','itemid_rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratingsDF2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Remove a rating if a user gives the same value for the same movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The join function combines two datasets (Key,ValueV) and (Key,ValueW) together to get (Key, (ValueV,ValueW)).  Let's join the movie and ratings file together to get meaningful recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'user_ratings_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-f01d33aee89a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'timeit unique_joined_ratings = user_ratings_data.filter(removeDuplicates)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2158\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2162\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2079\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2080\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2081\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2082\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0mnumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m                 \u001b[0mtime_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1058\u001b[0m                 \u001b[0mworst_tuning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworst_tuning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'user_ratings_data' is not defined"
     ]
    }
   ],
   "source": [
    "unique_joined_ratings = user_ratings_data.filter(removeDuplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_pairs = unique_joined_ratings.map(itemItem).partitionBy(numPartitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now group all ratings together for the same movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_pairs_ratings= movie_pairs.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms=[\"JACCARD\",\"COSINE\", \"PEARSON\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm =\"COSINE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if algorithm == algorithms[0] :\n",
    "\titem_item_similarities = movie_pairs_ratings.mapValues(jaccard_similarity).persist()\n",
    "if algorithm == algorithms[1]  :\n",
    "\titem_item_similarities = movie_pairs_ratings.mapValues(cosine_similarity).persist()\n",
    "elif algorithm == algorithms[2]  :\n",
    "\titem_item_similarities = movie_pairs_ratings.mapValues(pearson_similarity).persist()\n",
    "else:\n",
    "\titem_item_similarities = movie_pairs_ratings.mapValues(cosine_similarity).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the item pairs (each co-rated movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_item_sorted=item_item_similarities.sortByKey()\n",
    "item_item_sorted.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up your parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedmovie = sqlContext.sql('select  fro  movies limit 1')\n",
    "print(\"You have selected {0}\".format(movie.filter(movie_id)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = float(0.97)\n",
    "topN= int(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter for movies with this similarity that are \"good\" as defined by our quality thresholds above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredResults = item_item_sorted.filter(lambda((item_pair,similarity_occurence)): \\\n",
    "        (item_pair[0] == movie_id or item_pair[1] == movie_id) \\\n",
    "        and similarity_occurence[0] > threshold and similarity_occurence[1] > minOccurence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = filteredResults.map(lambda((x,y)): (y,x)).sortByKey(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsTopN = sc.parallelize(results.take(topN))\n",
    "resultsKey = resultsTopN.map(lambda((x,y)): (y[1],x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topMoviesJoin = resultsKey.join(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_N_Movies = topMoviesJoin.map(lambda (x,y): (y[0],(x,y[1][0].encode('ascii', 'ignore')))).sortByKey(ascending = False)\n",
    "top_N_Movies_Sorted = top_N_Movies.map(lambda (x,y): (y[1],y[0],x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_N_Movies_Sorted.saveAsTextFile(\"TOP10_cos1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "film_in_q = resultsTopN.map(lambda((x,y)): (y[0],x[0]))\n",
    "\n",
    "focus_join= film_in_q.join(movies)\n",
    "\n",
    "one_movie = focus_join.map(lambda (x,y): (y[1][0].encode('ascii', 'ignore'))).take(1)\n",
    "\n",
    "\n",
    "print(\"For movie:\" + str(one_movie) + \", the Top 10 recommended films are:\")\n",
    "top_N_DF = sqlContext.createDataFrame(top_N_Movies_Sorted, [\"Top 10 Recommended Movies(Year)\",\"Movie ID\",\"Similarity\"])\n",
    "top_N_DF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display data as a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotly.offline.init_notebook_mode()\n",
    "#plotly.offline.iplot(resultsTopN, filename='basic-heatmap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program was tested with the following versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext version_information\n",
    "%version_information numpy, scipy, matplotlib, pyspark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
